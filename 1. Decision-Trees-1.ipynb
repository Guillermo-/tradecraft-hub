{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "noble-repeat",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wireless-muscle",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -U scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "diverse-composer",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn import datasets\n",
    "from sklearn import model_selection\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import pandas\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "steady-merit",
   "metadata": {},
   "source": [
    "#### Mask the dataset creation for educational purposes (viewer: do not open)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "partial-specific",
   "metadata": {},
   "outputs": [],
   "source": [
    "data, labels = datasets.make_classification(n_samples=100,                                                                           n_features=5, n_classes=2, n_informative=3, n_redundant=0, n_repeated=0, random_state=7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interim-signature",
   "metadata": {},
   "source": [
    "#### Frame the data with features names for educational purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "designed-athletics",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data = pandas.DataFrame(data, columns=['comms', 'remote access', 'weather', 'external partners', 'physical access'])\n",
    "df_labels = pandas.DataFrame(labels, columns=['go/no-go criterion']) # go=0, no-go=1\n",
    "dataset = pandas.concat([df_data, df_labels], axis=1)  # put observations and label data together\n",
    "dataset.head()  # view some of the data for sanity check"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "colored-consultancy",
   "metadata": {},
   "source": [
    "#### We must now create a two datasets, one for training and another for testing the model afterwards. \n",
    "\n",
    "The training dataset provides our model with examples of the domain we are trying to capture and have the model learn some predicitive or classification capailities. \n",
    "\n",
    "This testing dataset will inform us how well the model performs on data it hasn't seen before, but is still within the domain in which we trained on. So we wouldn't train a model on identifying fruits and then test it on identifying cars. But the point of the testing data is to see how the model predict or classify in a general sense since this dataset is unseen examples. \n",
    "\n",
    "One point must be made. The fundamental assumption for machine learning practicioners is that our training and testing dataset represent the real-world (real domain) that we are trying to learn on. \n",
    "\n",
    "So below we split the whole dataset we have; 80% for training and 20% for testing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "convenient-entertainment",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train, data_test, labels_train, labels_test = model_selection.train_test_split(df_data, df_labels, test_size=0.2, random_state=7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "addressed-clothing",
   "metadata": {},
   "source": [
    "#### Let's see the training set that is recently split up now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "express-exhibition",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continental-regard",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "southwest-mortgage",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train.shape, labels_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "auburn-majority",
   "metadata": {},
   "source": [
    "#### And let's view the testing set that is recently split up now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gorgeous-tenant",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inclusive-clause",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "listed-dynamics",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test.shape, labels_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wrong-teaching",
   "metadata": {},
   "source": [
    "#### Now we finally create a Decision Tree model and use our training dataset for the model to learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "proved-magic",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_decision_tree = DecisionTreeClassifier(criterion='entropy', random_state=7)\n",
    "my_decision_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "determined-forty",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "superb-glucose",
   "metadata": {},
   "source": [
    "#### Using the Scikit Learn library we can train this model using its built-in `fit` function\n",
    "\n",
    "The fit function automatically trains our model. Isn't this a neat and easy implementation for us!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "illegal-development",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_decision_tree.fit(X=data_train, y=labels_train)\n",
    "my_decision_tree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spiritual-testament",
   "metadata": {},
   "source": [
    "#### That's it? What does it know right now? The Scikit Learn library has tools that enable us to visualize what the model has learned and how it will decide based on its internal function it developed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "infrared-willow",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ready-moral",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,10))\n",
    "tree.plot_tree(my_decision_tree, feature_names=data_train.columns, class_names=['go', 'no-go'], filled=True, proportion=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "streaming-locking",
   "metadata": {},
   "source": [
    "## Comments on the graph above\n",
    "\n",
    "First of all, the squares at the end of each tree branch are called leaf nodes. Each of these leaf nodes have a \"parent node,\" and the top-most node is called the root node. \n",
    "\n",
    " ***The root node*** <br>The root node contains the `comms` feature as its initial decision point. Why?\n",
    "\n",
    "\n",
    "The decision tree first determines which feature it should start with by using a metric to compare all other features relative to the go/no-go classification. In our model we used entropy\n",
    "\n",
    "Entropy comes from [information theory](https://en.wikipedia.org/wiki/Entropy_(information_theory)) and we'll use this metric on features to determine which one provides more information relative to the classification (i.e. go/no-go) and make it a decision pivot point (parent node). In our binary classification case of \"go/no-go\" the entropy equation will be as follows:\n",
    "\n",
    "$Entropy(S) =  -p_g \\log_2(p_g) - p_n \\log_2(p_n)$\n",
    "\n",
    "where <br>\n",
    "$p_g = \\frac{\\text {number of Go's}}{\\text {total number of observations in S}}$\n",
    "$p_n = \\frac{\\text {number of No-Go's}}{\\text {total number of observations in S}}$, \n",
    "and `S` is a set of observations (i.e. a group of examples)\n",
    "\n",
    "For example, if we have 14 examples in `S` of feature `comms` and 9 of them were labeled \"go\" and the other 5 were labeled \"no-go\", then the proportions are 9/14 (64%) Go's and 5/14 (36%) No-Go's. The Entropy equation results in </br>\n",
    "$Entropy_\\text{comms}(S) = -\\frac{9}{14} \\log_2 (\\frac{9}{14}) - \\frac{5}{14} \\log_2(\\frac{5}{14}) = 0.940$\n",
    "\n",
    "Now the Information Gain equation will use the Entropy metric on all of the features to determine which one results in the most information. The equation is as follows: </br>\n",
    "$Gain(S, F) = Entropy(S)  - \\displaystyle\\sum_{v \\in \\text{values in features F}} \\frac{S_v}{S} Entropy(S_v) $ </br>\n",
    "All this is saying is that we take the entropy of the whole set of observation `S` and subtract it from the entropy relative to the feature at hand. </br>\n",
    "\n",
    "Let's continue the example above to include features to illustrate. We know $Entropy(S)=0.94$, given 9 Go's and 5 No-Go's. \n",
    "$S = \\{9 \\mbox{Go's}\\} \\{5 \\mbox{No-Go's}\\} = 0.94$ </br>\n",
    "Note there are 14 total </br>\n",
    "Let's say that for the `weather` feature it can either take the value of `good` or `bad` and in this set `S` there were: </br>\n",
    "$S_\\text{weather=good} = \\{6 \\mbox{Go's}\\} \\{2 \\mbox{No-Go's}\\} $ <br>\n",
    "$S_\\text{weather=bad} = \\{3 \\mbox{Go's}\\} \\{3 \\mbox{No-Go's}\\} $ <br>\n",
    "Note that 8 total for good weather, and 6 total for bad weather <br>\n",
    "The information gain here would be: </br>\n",
    "$Gain(S, \\text{Weather}) = Entropy(S)  - \\displaystyle\\sum_{v \\in \\text{Good | Bad}} \\frac{S_v}{S} Entropy(S_v) $ <br>\n",
    "$Gain(S, \\text{Weather}) = Entropy(S) - (\\frac{8}{14})Entropy(S_\\text{weather=good}) - (\\frac{6}{14})Entropy(S_\\text{weather=bad})$ <br>\n",
    "$Gain(S, \\text{Weather}) = 0.94 - (\\frac{8}{14})(0.811) - (\\frac{6}{14})(1.00)$ <br>\n",
    "$Gain(S, \\text{Weather}) = 0.048 $ <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "formal-tennis",
   "metadata": {},
   "source": [
    "And so we would do this to all other features and choose the higher value. In our decision tree above we see that the `comms` entropy information gain was 1.0, and so if we compared it to this weather example above `comms` would win as well. [Reference](https://www.amazon.com/Machine-Learning-Tom-M-Mitchell/dp/0070428077) Tom Mitchell, Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "criminal-diabetes",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_accuracy = my_decision_tree.score(X=data_test, y=labels_test)\n",
    "print('Average Accuracy: {}%'.format(avg_accuracy * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "editorial-paint",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import plot_confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "multiple-semester",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(my_decision_tree, X=data_test, y_true=labels_test, display_labels=['go', 'no-go'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comfortable-demographic",
   "metadata": {},
   "source": [
    "#### (above) From the perspective of our model's prediction capability (columns), it did 9 correctly for `go` and mistakenly said `go` for 4 actual `no-go`s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lasting-salem",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(my_decision_tree, X=data_test, y_true=labels_test, display_labels=['go', 'no-go'], normalize='pred')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "danish-bunny",
   "metadata": {},
   "source": [
    "#### (above) This graph is the same but normalized based on the model's prediction (columns), and tell us that over this test dataset our model got \"confused\" 31% of the time in deciding to \"go\" as appropriate ***relative to the data***, whereas it was 100% not-confused for saying no-go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "short-dispatch",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "appropriate-welcome",
   "metadata": {},
   "source": [
    "### How can we understand more about our data to possibly know which features are informative? How can we determine which features are relevant to the machine learning model?\n",
    "\n",
    "### Lets jump to the `data-analysis-1` notebook to learning about informative features and some data structure. \n",
    "\n",
    "### Afterwards we'll jump to `feature-to-model-analysis-1` to determine what features are relavant to the machine learning model. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "velvet-start",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reverse-progressive",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "varying-steel",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
